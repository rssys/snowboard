#!/usr/bin/python3
"""
Concurrent input generator tool

This script generates test inputs for concurrency testing based on processor memory communication
(PMC) analysis. It has multiple strategies to create targeted test cases that should trigger 
race conditions and concurrency bugs in software.

input:
- - mem-dict-<timestamp>.txt: pickle file generated by mem-dict-generation.py containing the following structure. Format:

    mem_dict[memory_address][access_type][byte_length_index][instruction_ptr][value][double_read_flag] = [frequency, seed_set]
    
    where:
    - memory_address: virtual address accessed
    - access_type: integer flag (0=write, 1=read)
    - byte_length_index: integer index representing access width (0=8-bit, 1=16-bit, 2=32-bit)
    - instruction_ptr: hexadecimal address of the instruction that performed the memory operation
    - value: actual value read from or written to memory
    - double_read_flag: only present for reads (access_type=1), indicates whether this was part of a 
                        double-read pattern (0=normal read, 1=double read)
    - frequency: integer count of how many times this exact memory operation was observed
    - seed_set: set of test seed IDs where this memory operation was observed
    
    structure for writes:
    mem_dict[addr][0][byte_length_index][instruction_ptr][value]                   = [frequency, seed_set]
    
    structure for reads:
    mem_dict[addr][1][byte_length_index][instruction_ptr][value][double_read_flag] = [frequency, seed_set]
- PMC directory (PMC-*) from pmc-analysis.py: a directory containing PMC analysis files including:
    - uncommon-channel.txt: 
        all potential memory communications (PMCs) sorted by frequency, 
        representing shared memory access patterns where one thread writes to a location and 
        another reads from it. 
        Format:
        "<writer_instruction_addr> <writer_memory_addr> <write_byte_length> <reader_instruction_addr> 
        <reader_memory_addr> <read_byte_length> <frequency>"

    - uncommon-unaligned-channel.txt: 
        PMCs where memory access ranges differ between reader 
        and writer (different start addresses or lengths), which can cause bugs when readers 
        fetch partially updated data. 
        Format:
        "<writer_instruction_addr> <writer_memory_addr> <write_byte_length> <reader_instruction_addr> 
        <reader_memory_addr> <read_byte_length> <frequency>"

    - uncommon-double-read-channel.txt: 
        PMCs involving double-fetch patterns where the same 
        memory location is read twice in sequence, which can lead to time-of-check-to-time-of-use 
        vulnerabilities. 
        Format:
        "<writer_instruction_addr> <writer_memory_addr> <write_byte_length> <reader_instruction_addr> 
        <reader_memory_addr> <read_byte_length> <double_read_flag> <frequency>"

    - uncommon-object-null-channel.txt: 
        PMCs where a writer zeros out a shared object (writing value 0)
        that a reader later accesses, often leading to null pointer dereference bugs. 
        Format:
        "<writer_instruction_addr> <writer_memory_addr> <write_byte_length> <reader_instruction_addr> 
        <reader_memory_addr> <read_byte_length> <frequency>"

    - uncommon-ins-pair.txt: write-read instruction pairs involved in PMCs, highlighting 
        specific code patterns that may trigger concurrency bugs. 
        Format:
        "<writer_instruction_addr> <reader_instruction_addr> <frequency>"

    - uncommon-ins.txt: individual instructions (both reads and writes) participating in PMCs, 
        sorted by frequency to identify potentially vulnerable code. 
        Format:
        "<instruction_addr> <instruction_type{0=write,1=read}> <frequency>"

    - uncommon-mem-addr.txt: memory regions where inter-thread communication occurs, 
        helping identify shared kernel objects prone to race conditions. Format:
        "<writer_memory_addr> <reader_memory_addr> <write_byte_length> <read_byte_length> <frequency>"

strategies:
- M-CH: standard memory channel testing (channel_strategy)
- M-CH-DOUBLE: memory channels with double-read patterns (double_read_channel_strategy)
- M-CH-NULL: testing null object accesses (object_null_channel_strategy)
- M-CH-UNALIGNED: testing unaligned memory accesses (unaligned_channel_strategy)
- M-INS-PAIR: testing specific instruction pairs (ins_pair_strategy)
- M-INS: testing specific instructions (ins_strategy)
- M-OBJ: testing specific memory objects (mem_addr_strategy)

output:
- multiple log files (one per strategy) containing test cases for concurrency testing
- each file records details of memory operations, values, and seed information

each test case is a tuple containing:
[write_addr, read_addr, write_ins, read_ins, write_value, read_value,
 write_byte, read_byte, double_read, write_id, read_id]

usage:
  python generator.py [path_to_mem_dict_and_pmc_dir]

Interactive prompts will guide the selection of testing strategies.

Note: requires Redis server (default: 127.0.0.1:6380) for job queue management.
"""

import sys
import json
import os
from collections import defaultdict
import re
from multiprocessing import Process, Manager, Pool
import subprocess
import multiprocessing
import pickle
import time
from datetime import datetime
import random
from rq import Queue
from redis import Redis
import struct
from executor import concurrent_executor
from concurrent_common import TestCase

redis_host = '127.0.0.1'
redis_port = 6380
redis_passwd = 'snowboard-testing'

def set_to_list():
    return set([])

def write_ins_list():
    return [0, set([])]

def write_ins_new_dict():
    return defaultdict(write_ins_list)

def read_ins_list():
    return [[0, set([])], [0, set([])]]

def read_ins_new_dict():
    return defaultdict(read_ins_list)

def access_dict():
    return [[defaultdict(write_ins_new_dict), defaultdict(write_ins_new_dict), defaultdict(write_ins_new_dict)], 
            [defaultdict(read_ins_new_dict), defaultdict(read_ins_new_dict), defaultdict(read_ins_new_dict)]]

verbose_debugging = True

############################
# STRATEGIES               #
############################

def setup_strategy(pmc_info_path, log_path, strategy_name):
    time_now = datetime.now()
    timestamp = time_now.strftime("%Y-%m-%d-%H-%M-%S")
    result_filename = log_path + "/" + strategy_name + "-" + timestamp + ".txt"
    result_file = open(result_filename, 'w')
    redis_conn = Redis(host=redis_host, port=redis_port, password=redis_passwd)
    input_queue = Queue(strategy_name, connection=redis_conn)
    return result_file, input_queue

def enqueue_batch(testcase_pack, input_queue, strategy_name):
    if len(testcase_pack) > 0:
        #convert TestCase objects to lists before enqueuing
        list_testcase_pack = []
        for testcase in testcase_pack:
            list_testcase_pack.append(testcase.to_list())
        
        job = input_queue.enqueue(concurrent_executor, 
                                  args=(list_testcase_pack, strategy_name, False), 
                                  result_ttl=0, 
                                  job_timeout=366000)
    return []

def write_testcase_result(testcase, contents, result_file, strategy_name):
    frequency = 0
    if len(contents) > 6: #contents of type channel
        if strategy_name == "double-read":
            frequency = contents[7]  #double-read channel: frequency at index 7
        else:
            frequency = contents[6]  #regular channel: frequency at index 6
    else:
        #instruction-pair type (shorter format): frequency at index 2
        frequency = contents[2]
    
    print(testcase.format_for_log(frequency), file=result_file)
    result_file.flush()

def channel_strategy(pmc_info_path, log_path, max_queue_size):
    uncommon_channel_list = open(pmc_info_path + '/uncommon-channel.txt', 'r')
    result_file, input_queue = setup_strategy(pmc_info_path, log_path, "channel")
    total_input = 0
    while 1:
        while len(input_queue) < max_queue_size:
            testcase_pack = []
            while len(testcase_pack) < 500:
                channel_item = uncommon_channel_list.readline()
                if not channel_item:
                    print("Channel has generate all communications, about to return")
                    enqueue_batch(testcase_pack, input_queue, "channel")
                    return
                contents    = channel_item.strip().split(' ')
                write_ins   = int(contents[0], 16)
                write_addr  = int(contents[1], 16)
                write_byte  = int(contents[2]    )
                read_ins    = int(contents[3], 16)
                read_addr   = int(contents[4], 16)
                read_byte   = int(contents[5]    )
                total_input+= 1
                testcase    = channel_generator(write_ins, write_addr, write_byte, read_ins, read_addr, read_byte)
                testcase_pack.append(testcase)
                write_testcase_result(testcase, contents, result_file, "channel")   

            print("[channel] generated ", total_input, " inputs")
            enqueue_batch(testcase_pack, input_queue, "channel")

def object_null_channel_strategy(pmc_info_path, log_path, max_queue_size):
    uncommon_channel_list = open(pmc_info_path + '/uncommon-object-null-channel.txt', 'r')
    result_file, input_queue = setup_strategy(pmc_info_path, log_path, "object-null")
    total_input = 0
    while 1:
        while len(input_queue) < max_queue_size:
            testcase_pack = []
            while len(testcase_pack) < 500:
                channel_item = uncommon_channel_list.readline()
                if not channel_item:
                    print("object-null has generated all communications, about to return")
                    enqueue_batch(testcase_pack, input_queue, "object-null")
                    return
                contents    = channel_item.strip().split(' ')
                write_ins   = int(contents[0], 16)
                write_addr  = int(contents[1], 16)
                write_byte  = int(contents[2]    )
                read_ins    = int(contents[3], 16)
                read_addr   = int(contents[4], 16)
                read_byte   = int(contents[5]    )
                total_input+= 1
                testcase    = object_null_generator(write_ins, write_addr, write_byte, read_ins, read_addr, read_byte)
                testcase_pack.append(testcase)
                write_testcase_result(testcase, contents, result_file, "object-null")

            print("[object-null] generated ", total_input, " inputs")
            enqueue_batch(testcase_pack, input_queue, "object-null")

def double_read_channel_strategy(pmc_info_path, log_path, max_queue_size):
    uncommon_double_read_list = open(pmc_info_path + '/uncommon-double-read-channel.txt', 'r')
    result_file, input_queue = setup_strategy(pmc_info_path, log_path, "double-read")
    total_input = 0
    while 1:
        while len(input_queue) < max_queue_size:
            testcase_pack = []
            while len(testcase_pack) < 500:
                channel_item = uncommon_double_read_list.readline()
                if not channel_item:
                    print("double-read has generated all communications, about to return")
                    enqueue_batch(testcase_pack, input_queue, "double-read")
                    return
                contents    = channel_item.strip().split(' ')
                write_ins   = int(contents[0], 16)
                write_addr  = int(contents[1], 16)
                write_byte  = int(contents[2]    )
                read_ins    = int(contents[3], 16)
                read_addr   = int(contents[4], 16)
                read_byte   = int(contents[5]    )
                total_input+= 1
                testcase    = double_read_channel_generator(write_ins, write_addr, write_byte, read_ins, read_addr, read_byte)
                testcase_pack.append(testcase)
                write_testcase_result(testcase, contents, result_file, "double-read")

            print("[double-read] generated", total_input, "inputs")
            enqueue_batch(testcase_pack, input_queue, "double-read")

def unaligned_channel_strategy(pmc_info_path, log_path, max_queue_size):
    uncommon_channel_list = open(pmc_info_path + '/uncommon-unaligned-channel.txt', 'r')
    result_file, input_queue = setup_strategy(pmc_info_path, log_path, "unaligned-channel")
    total_input = 0
    while 1: 
        while len(input_queue) < max_queue_size:
            testcase_pack = []
            while len(testcase_pack) < 500:
                channel_item = uncommon_channel_list.readline()
                if not channel_item:
                    print("unaligned-channel has generated all communications, about to return")
                    enqueue_batch(testcase_pack, input_queue, "unaligned-channel")
                    return
                contents    = channel_item.strip().split(' ')
                write_ins   = int(contents[0], 16)
                write_addr  = int(contents[1], 16)
                write_byte  = int(contents[2]    )
                read_ins    = int(contents[3], 16)
                read_addr   = int(contents[4], 16)
                read_byte   = int(contents[5]    )
                total_input+= 1
                testcase    = channel_generator(write_ins, write_addr, write_byte, read_ins, read_addr, read_byte)
                testcase_pack.append(testcase)
                write_testcase_result(testcase, contents, result_file, "unaligned-channel")

            print("[unaligned-channel] generated ", total_input, " inputs")
            enqueue_batch(testcase_pack, input_queue, "unaligned-channel")

# Generates concurrent inputs based on ins-pair strategy
# pmc_info_path: the folder path where 'uncommon-ins-pair' is stored
# log_path: the folder path where generated concurrent inputs will be logged
# max_queue_size: the maximum number of concurrent input batches allowed. A batch of concurrent inputs contains 500 inputs.
def ins_pair_strategy(pmc_info_path, log_path, max_queue_size):
    uncommon_ins_pair_list = open(pmc_info_path + '/uncommon-ins-pair.txt', 'r')
    result_file, input_queue = setup_strategy(pmc_info_path, log_path, "ins-pair")
    total_input = 0
    while 1:
        while len(input_queue) < max_queue_size:
            # find a batch for testing
            testcase_pack = []
            while len(testcase_pack) < 500:
                ins_pair_item = uncommon_ins_pair_list.readline()
                if not ins_pair_item:
                    print("INS-PAIR strategy has generated all concurrent inputs")
                    # push the rest testcase_pack
                    enqueue_batch(testcase_pack, input_queue, "ins-pair")
                    return
                contents        = ins_pair_item.strip().split(' ')
                write_ins       = int(contents[0], 16)
                read_ins        = int(contents[1], 16)
                key             = tuple([write_ins, read_ins])
                assert(key in ins_pair_to_channel)
                mem_area_list   = list(ins_pair_to_channel[key])
                mem_area        = random.choice(mem_area_list)
                mem_area        = list(mem_area)
                write_addr      = mem_area[0]
                write_byte      = mem_area[1]
                read_addr       = mem_area[2]
                read_byte       = mem_area[3]
                total_input    += 1
                testcase        = channel_generator(write_ins, write_addr, write_byte, read_ins, read_addr, read_byte)
                testcase_pack.append(testcase)
                write_testcase_result(testcase, contents, result_file, "ins-pair")

            print("[ins-pair] generating ", total_input, " inputs")
            enqueue_batch(testcase_pack, input_queue, "ins-pair")

def ins_strategy(pmc_info_path, log_path, max_queue_size):
    uncommon_ins_list = open(pmc_info_path + '/uncommon-ins.txt', 'r')
    result_file, input_queue = setup_strategy(pmc_info_path, log_path, "ins")
    total_input = 0
    while 1:
        while len(input_queue) < max_queue_size:
            # find a batch for testing
            testcase_pack = []
            while len(testcase_pack) < 500:
                ins_item = uncommon_ins_list.readline()
                if not ins_item:
                    enqueue_batch(testcase_pack, input_queue, "ins")
                    return
                try:
                    contents = ins_item.strip().split(' ')
                except:
                    print("Error with line", ins_item)
                    continue
                ins = int(contents[0], 16)
                ins_type = int(contents[1])
                if ins_type == 0:
                    key = tuple([ins, 0])
                    assert(key in write_ins_to_channel)
                    if key not in write_ins_to_channel:
                        print("error happen: a strange write instruction that Snowboard does not know", key)
                        exit(1)
                    mem_area_list   = list(write_ins_to_channel[key])
                    mem_area        = list(random.choice(mem_area_list))
                    write_ins       = ins
                    read_ins        = mem_area[0]
                    write_addr      = mem_area[1]
                    write_byte      = mem_area[2]
                    read_addr       = mem_area[3]
                    read_byte       = mem_area[4]
                elif ins_type == 1:
                    key = tuple([ins, 1])
                    if key not in read_ins_to_channel:
                        print("error happen: a strange read instruction that Snowboard does not know", key)
                        exit(1)
                    mem_area_list   = list(read_ins_to_channel[key])
                    mem_area        = list(random.choice(mem_area_list))
                    write_ins       = mem_area[0]
                    read_ins        = ins
                    write_addr      = mem_area[1]
                    write_byte      = mem_area[2]
                    read_addr       = mem_area[3]
                    read_byte       = mem_area[4]
                total_input        += 1
                testcase            = channel_generator(write_ins, write_addr, write_byte, read_ins, read_addr, read_byte) 
                testcase_pack.append(testcase)
                write_testcase_result(testcase, contents, result_file, "ins")

            print("[ins] generated", total_input, " inputs")
            enqueue_batch(testcase_pack, input_queue, "ins")

def mem_addr_strategy(pmc_info_path, log_path, max_queue_size):
    uncommon_mem_area_list = open(pmc_info_path + '/uncommon-mem-addr.txt', 'r')
    result_file, input_queue = setup_strategy(pmc_info_path, log_path, "mem-addr")
    total_input = 0
    while 1:
        while len(input_queue) < max_queue_size:
            testcase_pack = []
            while len(testcase_pack) < 500:
                mem_area_item = uncommon_mem_area_list.readline()
                if not mem_area_item:
                    print("mem-addr has generated all communications, about to return")
                    enqueue_batch(testcase_pack, input_queue, "mem-addr")
                    return
                contents    = mem_area_item.strip().split(' ')
                write_addr  = int(contents[0], 16)
                read_addr   = int(contents[1], 16)
                write_byte  = int(contents[2])
                read_byte   = int(contents[3])
                total_input+= 1
                testcase    = mem_area_generator(write_addr, write_byte, read_addr, read_byte)
                testcase_pack.append(testcase)
                write_testcase_result(testcase, contents, result_file, "mem-addr")

            print("[mem-addr] generated", total_input, "inputs")
            enqueue_batch(testcase_pack, input_queue, "mem-addr")

############################
# GENERATOR                #
############################

def setup_memory_addresses(write_addr, write_byte, read_addr, read_byte):
    access_byte_to_index = {1:0, 2:1, 4:2}
    write_access_dict    = mem_dict[write_addr][0]
    read_access_dict     = mem_dict[read_addr][1]
    write_length_index   = access_byte_to_index[write_byte]
    read_length_index    = access_byte_to_index[read_byte]
    
    write_addr_begin     = write_addr
    write_addr_end       = write_addr + write_byte - 1
    read_addr_begin      = read_addr
    read_addr_end        = read_addr + read_byte - 1
    write_addr_set       = set([])
    read_addr_set        = set([])
    for addr in range(write_addr_begin, write_addr_end + 1):
        write_addr_set.add(addr)
    for addr in range(read_addr_begin, read_addr_end + 1):
        read_addr_set.add(addr)
    
    # check if two memory access could overlap
    assert(len(write_addr_set & read_addr_set) != 0)
    addr_begin           = min(list(write_addr_set & read_addr_set))
    addr_end             = max(list(write_addr_set & read_addr_set))
    read_begin_offset    = addr_begin - read_addr_begin
    read_end_offset      = addr_end - read_addr_begin + 1
    write_begin_offset   = addr_begin - write_addr_begin
    write_end_offset     = addr_end - write_addr_begin + 1
    
    return (write_access_dict, read_access_dict, write_length_index, read_length_index,
            read_begin_offset, read_end_offset, write_begin_offset, write_end_offset)

def get_value_dicts(write_access_dict, read_access_dict, write_length_index, read_length_index, write_ins, read_ins):
    write_ins_dict       = write_access_dict[write_length_index]
    read_ins_dict        = read_access_dict[read_length_index]
    write_value_dict     = write_ins_dict[write_ins]
    read_value_dict      = read_ins_dict[read_ins]
    return write_value_dict, read_value_dict

def setup_memory_operations(write_addr, write_byte, read_addr, read_byte, write_ins, read_ins):
    (write_access_dict, read_access_dict, write_length_index, read_length_index,
     read_begin_offset, read_end_offset, write_begin_offset, write_end_offset) = setup_memory_addresses(
         write_addr, write_byte, read_addr, read_byte)
    
    write_value_dict, read_value_dict = get_value_dicts(
        write_access_dict, read_access_dict, write_length_index, read_length_index, write_ins, read_ins)
    
    return (write_value_dict, read_value_dict, 
            read_begin_offset, read_end_offset, 
            write_begin_offset, write_end_offset)

def create_testcase(write_addr, read_addr, write_ins, read_ins, write_value, read_value, 
                   write_byte, read_byte, write_candidate_list, read_candidate_list):
    double_read_freq = write_candidate_list[0] * read_candidate_list[1][0]
    normal_freq      = write_candidate_list[0] * read_candidate_list[0][0]
    if double_read_freq > 0 and normal_freq > 0:
        # randonly decide if we try double-read first
        try_double_read = random.choice([0, 1])
        if try_double_read:
            write_id_list = list(write_candidate_list[1])
            read_id_list  = list(read_candidate_list[1][1])
            write_id      = random.choice(write_id_list)
            read_id       = random.choice(read_id_list)
            return TestCase(write_addr, read_addr, write_ins, read_ins, write_value, 
                           read_value, write_byte, read_byte, 1, write_id, read_id)
        else:
            write_id_list = list(write_candidate_list[1])
            read_id_list  = list(read_candidate_list[0][1])
            write_id      = random.choice(write_id_list)
            read_id       = random.choice(read_id_list)
            return TestCase(write_addr, read_addr, write_ins, read_ins, write_value, 
                           read_value, write_byte, read_byte, 0, write_id, read_id)
    
    if double_read_freq > 0 and normal_freq == 0:
        write_id_list     = list(write_candidate_list[1])
        read_id_list      = list(read_candidate_list[1][1])
        write_id          = random.choice(write_id_list)
        read_id           = random.choice(read_id_list)
        return TestCase(write_addr, read_addr, write_ins, read_ins, write_value, 
                       read_value, write_byte, read_byte, 1, write_id, read_id)
    
    if double_read_freq == 0 and normal_freq > 0:
        write_id_list     = list(write_candidate_list[1])
        read_id_list      = list(read_candidate_list[0][1])
        write_id          = random.choice(write_id_list)
        read_id           = random.choice(read_id_list)
        return TestCase(write_addr, read_addr, write_ins, read_ins, write_value, 
                       read_value, write_byte, read_byte, 0, write_id, read_id)

def values_differ(write_value, read_value, write_begin_offset, write_end_offset, 
                 read_begin_offset, read_end_offset):
    write_bytes_value     = struct.pack('<I', write_value)
    write_actual_value    = write_bytes_value[write_begin_offset: write_end_offset]
    read_bytes_value      = struct.pack('<I', read_value)
    read_actual_value     = read_bytes_value[read_begin_offset:read_end_offset]
    return write_actual_value != read_actual_value

def randomize_dict_keys(dict):
    dict_list = list(dict.keys())
    random.shuffle(dict_list)
    return dict_list

def channel_generator(write_ins, write_addr, write_byte, read_ins, read_addr, read_byte):
    (write_value_dict, read_value_dict, 
     read_begin_offset, read_end_offset, 
     write_begin_offset, write_end_offset) = setup_memory_operations(
         write_addr, write_byte, read_addr, read_byte, write_ins, read_ins)
    
    random_write_value_list = randomize_dict_keys(write_value_dict)
    random_read_value_list  = randomize_dict_keys(read_value_dict)

    for write_value in random_write_value_list:
        for read_value in random_read_value_list:
            if values_differ(write_value, read_value, write_begin_offset, write_end_offset, 
                             read_begin_offset, read_end_offset):
                write_candidate_list = write_value_dict[write_value]
                read_candidate_list = read_value_dict[read_value]
                return create_testcase(write_addr, read_addr, write_ins, read_ins, write_value, read_value, 
                                      write_byte, read_byte, write_candidate_list, read_candidate_list)

def object_null_generator(write_ins, write_addr, write_byte, read_ins, read_addr, read_byte):
    (write_value_dict, read_value_dict, 
     read_begin_offset, read_end_offset, 
     write_begin_offset, write_end_offset) = setup_memory_operations(
         write_addr, write_byte, read_addr, read_byte, write_ins, read_ins)
    
    random_write_value_list = randomize_dict_keys(write_value_dict)
    random_read_value_list  = randomize_dict_keys(read_value_dict)
    if 0 not in write_value_dict:
        print("No communication nullificate the memory address")
        exit(1)
    random_write_value_list = [0]

    for write_value in random_write_value_list:
        for read_value in random_read_value_list:
            if values_differ(write_value, read_value, write_begin_offset, write_end_offset, 
                             read_begin_offset, read_end_offset):
                write_candidate_list = write_value_dict[write_value]
                read_candidate_list  = read_value_dict[read_value]
                return create_testcase(write_addr, read_addr, write_ins, read_ins, write_value, read_value, 
                                    write_byte, read_byte, write_candidate_list, read_candidate_list)

def double_read_channel_generator(write_ins, write_addr, write_byte, read_ins, read_addr, read_byte):
    (write_value_dict, read_value_dict, 
     read_begin_offset, read_end_offset, 
     write_begin_offset, write_end_offset) = setup_memory_operations(
         write_addr, write_byte, read_addr, read_byte, write_ins, read_ins)
    
    random_write_value_list = randomize_dict_keys(write_value_dict)
    random_read_value_list  = randomize_dict_keys(read_value_dict)

    for write_value in random_write_value_list:
        for read_value in random_read_value_list:
            if values_differ(write_value, read_value, write_begin_offset, write_end_offset, 
                             read_begin_offset, read_end_offset):
                write_candidate_list = write_value_dict[write_value]
                read_candidate_list  = read_value_dict[read_value]
                freq = write_candidate_list[0] * read_candidate_list[1][0]
                if freq:
                    write_id_list    = list(write_candidate_list[1])
                    read_id_list     = list(read_candidate_list[1][1])
                    write_id         = random.choice(write_id_list)
                    read_id          = random.choice(read_id_list)
                    return TestCase(write_addr, read_addr, write_ins, read_ins, write_value, 
                                   read_value, write_byte, read_byte, 1, write_id, read_id)

def mem_area_generator(write_addr, write_byte, read_addr, read_byte):
    (write_access_dict, read_access_dict, write_length_index, read_length_index,
     read_begin_offset, read_end_offset, write_begin_offset, write_end_offset) = setup_memory_addresses(
         write_addr, write_byte, read_addr, read_byte)

    write_ins_dict        = write_access_dict[write_length_index]
    read_ins_dict         = read_access_dict[read_length_index]

    random_write_ins_list = randomize_dict_keys(write_ins_dict)
    random_read_ins_list  = randomize_dict_keys(read_ins_dict)

    for write_ins in random_write_ins_list:
        for read_ins in random_read_ins_list:
            write_value_dict, read_value_dict = get_value_dicts(
                write_access_dict, read_access_dict, write_length_index, read_length_index, write_ins, read_ins)
            
            random_write_value_list = randomize_dict_keys(write_value_dict)
            random_read_value_list  = randomize_dict_keys(read_value_dict)
            
            for write_value in random_write_value_list:
                for read_value in random_read_value_list:
                    if values_differ(write_value, read_value, write_begin_offset, write_end_offset, 
                                     read_begin_offset, read_end_offset):
                        write_candidate_list = write_value_dict[write_value]
                        read_candidate_list  = read_value_dict[read_value]
                        return create_testcase(write_addr, read_addr, write_ins, read_ins, write_value, read_value, 
                                              write_byte, read_byte, write_candidate_list, read_candidate_list)

############################
# CHANNEL                  #
############################

def open_channel_file(pmc_info_path):
    uncommon_channel_list = open(pmc_info_path + '/uncommon-channel.txt', 'r')
    channel_lines = uncommon_channel_list.readlines()
    uncommon_channel_list.close()
    return channel_lines

def parse_channel_line(line):
    contents    = line.strip().split(' ')
    write_ins   = int(contents[0], 16)
    write_addr  = int(contents[1], 16)
    write_byte  = int(contents[2])
    read_ins    = int(contents[3], 16)
    read_addr   = int(contents[4], 16)
    read_byte   = int(contents[5])
    return write_ins, write_addr, write_byte, read_ins, read_addr, read_byte

def add_to_set(mapping, key, value):
    if key not in mapping:
        mapping[key] = set([])
    mapping[key].add(value)

# Build a mapping from every unique ins-pair to a list of PMC channels it belongs to
def ins_pair_2_channel(pmc_info_path):
    time_start = time.time()
    ins_pair_to_channel = defaultdict(set_to_list)
    if verbose_debugging:
        print("Building a mapping from ins-pair to the channel it belongs to")
    
    for line in open_channel_file(pmc_info_path):
        write_ins, write_addr, write_byte, read_ins, read_addr, read_byte = parse_channel_line(line)
        key   = tuple([write_ins, read_ins])
        value = tuple([write_addr, write_byte, read_addr, read_byte])
        add_to_set(ins_pair_to_channel, key, value)
        
    time_end = time.time()
    if verbose_debugging:
        print("Num of unique ins-pairs:", len(ins_pair_to_channel.keys()))
        print('time cost',time_end-time_start, 's')
    
    return ins_pair_to_channel

# Build a mapping from a unique ins to a list of PMC channels it belongs to
def ins_2_channel(pmc_info_path):
    time_start           = time.time()
    write_ins_to_channel = defaultdict(set_to_list)
    read_ins_to_channel  = defaultdict(set_to_list)
    if verbose_debugging:
        print("Building a mapping from a ins to the channel it belongs to")
    
    for line in open_channel_file(pmc_info_path):
        write_ins, write_addr, write_byte, read_ins, read_addr, read_byte = parse_channel_line(line)
        
        write_key   = tuple([write_ins, 0])
        write_value = tuple([read_ins, write_addr, write_byte, read_addr, read_byte])
        add_to_set(write_ins_to_channel, write_key, write_value)
        
        read_key    = tuple([read_ins, 1])
        read_value  = tuple([write_ins, write_addr, write_byte, read_addr, read_byte])
        add_to_set(read_ins_to_channel, read_key, read_value)
    
    time_end = time.time()
    if verbose_debugging:
        print("Number of unique write ins", len(write_ins_to_channel.keys()), "unique read ins", len(read_ins_to_channel.keys())) 
        print('time cost',time_end-time_start, 's')
    
    return write_ins_to_channel, read_ins_to_channel

############################
# MAIN                     #
############################

def display_strategy_menu():
    prompt_msg = "The generator can generate concurrent inputs based on the following strategies:\n[1]: M-CH strategy\n[2]: M-CH-DOUBLE strategy\n[3]: M-CH-NULL strategy\n[4]: M-CH-UNALIGNED strategy\n[5]: M-INS-PAIR strategy\n[6]: M-INS strategy\n[7]: M-OBJ strategy"
    print(prompt_msg)
    strategy_choice_input = input("Please select strategy(s) by entering the strategy ID(s) and delimiting them by a space (e.g. 1 2 3)\n")
    return list(map(int, strategy_choice_input.split()))

def find_file(base_path, name_pattern):
    cmd = f'find {base_path} -name "{name_pattern}"'
    out = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE, 
                           stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = out.communicate()
    return stdout.decode().strip()

def load_mem_dict(mem_dict_filename):
    print("Loading python dictionary into memory")
    with open(mem_dict_filename, 'rb') as mem_dict_file:
        return pickle.load(mem_dict_file)

def setup_channel_mappings(pmc_dir):
    ins_pair_to_channel                  = ins_pair_2_channel(pmc_dir)
    write_ins_to_channel, read_ins_to_channel = ins_2_channel(pmc_dir)
    return ins_pair_to_channel, write_ins_to_channel, read_ins_to_channel

def setup_output_directory():
    time_now            = datetime.now()
    timestamp           = time_now.strftime("%Y-%m-%d-%H-%M-%S")
    snowboard_storage   = os.environ.get('SNOWBOARD_STORAGE')
    if snowboard_storage is None:
        print("[Error] Please source scripts/setup.sh $FOLDER_TO_STORE_DATA first")
        exit(1)
    output_dir = snowboard_storage + '/generator-log-' + timestamp + '/'
    if not os.path.isdir(output_dir):
        os.makedirs(output_dir)
    return output_dir

def run_strategies(strategy_indices, pmc_dir, output_dir, max_queue_size=10):
    strategy_list = [
        channel_strategy, 
        double_read_channel_strategy, 
        object_null_channel_strategy, 
        unaligned_channel_strategy,
        ins_pair_strategy,
        ins_strategy, 
        mem_addr_strategy
    ]
    
    process_pool = Pool(len(strategy_indices))
    result_list = []
    for index in strategy_indices:
        result = process_pool.apply_async(strategy_list[index-1], 
                                          args=(pmc_dir, output_dir, max_queue_size))
        result_list.append(result)
    
    for result in result_list:
        result.get()
    
    process_pool.close()
    process_pool.join()

def main():
    global mem_dict, ins_pair_to_channel, write_ins_to_channel, read_ins_to_channel
    
    strategy_choice     = display_strategy_menu()
    
    base_path           = sys.argv[1]
    mem_dict_filename   = find_file(base_path, "mem-dict-*")
    pmc_dir             = find_file(base_path, "PMC-*")
    print(mem_dict_filename, pmc_dir)
    
    mem_dict            = load_mem_dict(mem_dict_filename)
    ins_pair_to_channel, write_ins_to_channel, read_ins_to_channel = setup_channel_mappings(pmc_dir)
    
    output_dir          = setup_output_directory()
    
    run_strategies(strategy_choice, pmc_dir, output_dir)

if __name__ == "__main__":
    main()
